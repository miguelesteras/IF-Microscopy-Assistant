%% Phase 5.3 Train an AutoEncoder from Summary Image
%   ======================================================================
%   Code by Miguel Esteras-Bejar, 07/2017
%   This code is part of the project:
%   'Tracking of temporally occluded or overlapping structures in live cell
%   microscopy'
%   This codes aims to:
%   1. Train a AutoEncoder network for summary image input, followed by a 
%   fully connected layer to form a output frame. 
%   ======================================================================

rng('default')

% Create a joined dataSet from all video files
files = dir('*_metadata.mat');      
num_files = length(files);
inputData = [];   
targetData = [];
for i = 1:num_files
    load(files(i).name,'metadata');                               
    load(strcat(metadata.name,'_dynamicInput.mat'),'dynamicInput');     
    load(strcat(metadata.name,'_dynamicTarget.mat'),'dynamicTarget');     
    inputData = [inputData dynamicInput'];
    targetData = [targetData dynamicTarget'];
end

%% reduce size of images to reduce size of Autoencoder
load(strcat(metadata.name,'_maxRadii.mat'),'maxRadii');     
index = find(~cellfun(@isempty,maxRadii)); % non empty cells in cell sequences array
radious = ceil(cellfun(@(v) v(2), maxRadii(index)));    
%histogram(radious)

cropData = cellfun(@(x) imcrop(x,[50 35 93 123]),inputData,...
    'UniformOutput',false);

%% Train a autoencoder for dynamic images
%% First layer
enco_fnc = ['logsig' 'satlin'];
deco_fnc = ['logsig' 'satlin' 'purelin'];

hiddenSize1 = 1000; tic;                         	% start timer
AE1 = trainAutoencoder(cropData,hiddenSize1, ... % images in cell array format
    'MaxEpochs',1000, ...                       % max no.epochs for early stopping
    'EncoderTransferFunction', 'logsig',...     % logistic sigmoid function
    'DecoderTransferFunction', 'logsig',...     % logistic sigmoid function
    'LossFunction', 'msesparse',...             % loss function used for training
    'TrainingAlgorithm', 'trainscg',...         % training algorithm, scaled conjugate gradient descent
    'L2WeightRegularization',0.004, ...         % coefficient for the L2 weight regularizer in the cost function
    'SparsityRegularization',4, ...             % controls the weighting of the sparsity regularizer
    'SparsityProportion',0.2, ...               % proportion of training examples which a neuron in the hidden layer should activate in response to.
    'ScaleData', false);                        % do not rescale input data

view(AE1)                                  % visualize autoencoder1
plotWeights(AE1);                          % visualize feautres from autoencoder1 hidden layer
sprintf('Training time AE1: %0.0f %s',toc,'s')  % duration of training

% i = 100;
% Reconstructed = predict(autoenc1,Xtrain{i});    % code -> decode image 1 to 10
% figure; subplot(1,2,1); imshow(Xtrain{i});      % show original image
% subplot(1,2,2); imshow(Reconstructed);          % show reconstracted image

AE1features = encode(AE1,cropData);            % generate feautres from autoencoder1 hidden layer



